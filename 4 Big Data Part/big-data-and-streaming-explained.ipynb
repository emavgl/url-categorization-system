{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Big Data Analysis\n",
    "\n",
    "In the data mining part of the project we found different methods to make the topic modelling computation less expensive, when the user changes the granularity of the grid. However, since we have to deal with dataset which contains log of http requests we have to analyze a large amount of data. In this part of the project we want to adapt our data mining algorithm to be performed in a parallel on a cluster of computers.\n",
    "\n",
    "### The algorithm\n",
    "\n",
    "The *baseline*, *lda update* algorithms are difficult to parallelize because it is necessary to maintain a read and write *lda model object* updated thorough the cluster. The algorithm *top topic aggregation* is more suited to our studies because it doesn't need to mantain a shared object updated but it only needs to access a shared read-only lda models, which we can distribute to the workers beforehand, and access from the file system.\n",
    "\n",
    "#### Remarks\n",
    "The *top topic aggregation* algorithm takes as input the *lda model* that holds for the whole grid, and the new grid (a partitioning of the documents) whose topics we wish to compute. Then, since each cell of the new grid is composed of multiple cells of the old grid, it computes the topics distributions for each cell of the old grid and then merges these distributions together in order to obtain the topic distributions for the new cells.\n",
    "\n",
    "### Tools and Big Data adaptation\n",
    "We use the tool *Apache Spark* in *python* so that we can reuse the same modules that we used in the data mining part. *Apache Spark* is a framework which contains API to deal with big data, data streaming processing and clustering computing. Apache Spark supports in-memory computing and different distributed file systems, between which *Apache Hadoop Distributed File System (HDFSâ„¢)*.\n",
    "\n",
    "To adapt the algorithm *top topic aggregation*, we use the *MapReduce* programming model. To apply it, it is needed a pre-processing of the input in order to represent the old grid as a list of tuples (key, value). Where the key is the index of the cell of the old grid, and the value is the set of contained documents. Each key, ID of the cell of the old grid, is then replaced with ID of the cell of the new grid containing it.\n",
    "\n",
    "This list is the input of the MapReduce algorithm that first maps each tuple into a new tuple (key, topic_distributions(documents)) computing the topic distributions for each subset of documents using the precomputed lda model of whole grid. The tuples with the same key are then aggregated together with the reduce operation  *top_topic_merge*, obtaining, for each cell of the new grid, a topic distribution. Refer to the data-mining part of the project for an explanation of *top_topic_merge*.\n",
    "\n",
    "![mapReduce](img/mapreduce.png \"MapReduce Algorithm\")\n",
    "\n",
    "### Results\n",
    "\n",
    "The algorithm has a linear complexity O(n), the performances differs by a factor that depends on the number of machines, processor and cores used. The chart below represents the results obtained run the algorithm in a single machine and in a cluster of 4 worker machines with 4 cores, 2.4 GHz Intel Xeon processor.\n",
    "\n",
    "The best results are obtained with the cluster of computers. It computed the topics for 20k documents in 20 seconds, 100k in 70 seconds and for 200k documents in 153 seconds.\n",
    "\n",
    "![result](img/results.png \"Results\")\n",
    "\n",
    "## Dealing with Streaming of Data\n",
    "\n",
    "The problem that we are studying is about a continuos flow of data and, if we are interested in computing the topics on the fly, a streaming algorithm is needed. Sparks provides also streaming processing API that let us easily adapt the MapReduce algorithm to work with a continuos streaming of data.\n",
    "\n",
    "The program *tweet_read.py* starts a streaming of tweets which are sent to the spark program via socket. \n",
    "\n",
    "```python\n",
    "# Spark code receiving the streaming data\n",
    "ssc = StreamingContext(sc, 30)\n",
    "lines = ssc.socketTextStream(\"localhost\", 5727)\n",
    "```\n",
    "\n",
    "the variable *lines* represents a *discretized stream (DStream)* which is an high-level abstraction that hides the complexity of dealing with a continuous data stream and makes it as easy for programmers as using one single RDD at a time. [1]\n",
    "\n",
    "![streaming](img/streaming.png \"Streaming Workflow\")\n",
    "\n",
    "\n",
    "\n",
    "[1] https://www.gitbook.com/book/jaceklaskowski/spark-streaming/details\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# standard modules \n",
    "import time\n",
    "import json\n",
    "import pickle\n",
    "import logging\n",
    "\n",
    "# spark modules\n",
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "\n",
    "# custom modules\n",
    "from lib.lda import LDAHelper\n",
    "\n",
    "# third party modules\n",
    "import shapely\n",
    "from shapely import geometry\n",
    "import preprocessor as twpr\n",
    "import pprint\n",
    "\n",
    "# init spark\n",
    "try:\n",
    "    sc.stop()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "sc = SparkContext('local[2]', appName='test')\n",
    "\n",
    "print(\"Spark context created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# LDA Helper\n",
    "lda_helper = LDAHelper(15)\n",
    "\n",
    "# import dictionary\n",
    "with open('data/dictionary.pkl', 'rb') as f:\n",
    "    dictionary = pickle.load(f)\n",
    "    \n",
    "# Get files back\n",
    "new_grid_filename1 = \"data/input_1_new_grid_part1.pkl\"\n",
    "new_grid_filename2 = \"data/input_1_new_grid_part2.pkl\"\n",
    "lda_map_filename = \"data/input_2_lda_map.pkl\"\n",
    "\n",
    "new_grid = []\n",
    "with open(new_grid_filename1, 'rb') as fw:\n",
    "    new_grid = pickle.load(fw)\n",
    "\n",
    "with open(new_grid_filename2, 'rb') as fw:\n",
    "    new_grid += pickle.load(fw)\n",
    "\n",
    "lda_map = None\n",
    "with open(lda_map_filename, 'rb') as fw:\n",
    "    lda_map = pickle.load(fw)\n",
    "    \n",
    "print(\"Data files loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Topic using Top Topic Aggregation with Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Compute the spark_grid\n",
    "spark_grid = []\n",
    "counter = 0\n",
    "for cell in new_grid:\n",
    "    cell_id = counter\n",
    "    for p in cell['parts']:\n",
    "        corpus = p['corpus']\n",
    "        tup = (cell_id, corpus)\n",
    "        spark_grid.append(tup)\n",
    "    counter += 1\n",
    "\n",
    "lda_map_broadcast = sc.broadcast(lda_map)\n",
    "\n",
    "# So the first thing that we have to do is convert a list in a RDD\n",
    "start_time = time.time()\n",
    "\n",
    "rdd_grid = sc.parallelize(spark_grid, 4)\n",
    "topics = rdd_grid.map(lambda scell: (scell[0], lda_helper.calculate_topic_distributions(lda_map_broadcast.value, scell[1])))\n",
    "topics = topics.reduceByKey(lambda a, b: lda_helper.merge_toptopic(a, b))\n",
    "topics = topics.sortByKey(True)\n",
    "spark_result = topics.collect()\n",
    "spark_result = list(zip(*spark_result))[1]\n",
    "spark_result = list(spark_result)\n",
    "\n",
    "# spark results contains the topics\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "# integrate topics into spark_grid\n",
    "for i, cell in enumerate(new_grid):\n",
    "    cell['topic'] = spark_result[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def find_cell(shape_tweet):\n",
    "    \"\"\"\n",
    "    Find the cell that contains the tweet\n",
    "    \"\"\"\n",
    "    for i, cell in enumerate(new_grid):\n",
    "        pprint.pprint('cell polygon')\n",
    "        pprint.pprint(type(cell['polygon']))\n",
    "        pprint.pprint('shape_tweet')\n",
    "        pprint.pprint(type(shape_tweet))\n",
    "        if cell[\"polygon\"].contains(shape_tweet):\n",
    "            return i\n",
    "    return -1\n",
    "\n",
    "def filter_tweet(tweet_raw):\n",
    "    \"\"\"\n",
    "    Remove tweets without geo-coordinates\n",
    "    or out of our maps\n",
    "    \"\"\"\n",
    "    tweet = json.loads(tweet_raw)\n",
    "    coordinates = tweet['coordinates']\n",
    "    if not coordinates or coordinates == 'null':\n",
    "        coordinates = (tweet['place'])['bounding_box']\n",
    "        if not coordinates or coordinates == 'null': return False\n",
    "    shape_tweet = geometry.shape(coordinates)\n",
    "    cell_id = find_cell(shape_tweet)\n",
    "    if cell_id == -1: return False\n",
    "    return True\n",
    "    \n",
    "def evaluate_tweet(tweet):\n",
    "    \"\"\"\n",
    "    Convert the text in a tweet into a bag of words object\n",
    "    \"\"\"\n",
    "    tweet = json.loads(tweet)\n",
    "    text = tweet['text']\n",
    "    coordinates = tweet['coordinates']\n",
    "    if not coordinates or coordinates == 'null':\n",
    "        coordinates = (tweet['place'])['bounding_box']\n",
    "    shape_tweet = shapely.geometry.shape(coordinates)\n",
    "    text = twpr.clean(text)\n",
    "    content = lda_helper.clean(text)\n",
    "    tweet_corpus = lda_helper.create_corpus([content], dictionary)\n",
    "    topic_dist = lda_helper.calculate_topic_distributions(lda_map, tweet_corpus)\n",
    "    cell_id = find_cell(shape_tweet)\n",
    "    return (cell_id, topic_dist)\n",
    "\n",
    "def final_aggregations(grid, topics):\n",
    "    \"\"\"\n",
    "    Aggregate top topics distributions\n",
    "    with the existing topics distributions\n",
    "    in the grid\n",
    "    \"\"\"\n",
    "    for key, value in topics:\n",
    "        grid[key]['topic'] = lda_helper.merge_toptopic(grid[key]['topic'], value)\n",
    "\n",
    "def evaluate_rdd(topics):\n",
    "    \"\"\"\n",
    "    Collect from RDD and call final_aggregations\n",
    "    \"\"\"\n",
    "    if topics.isEmpty():\n",
    "        print('is empty')\n",
    "    else:\n",
    "        print('not empty')\n",
    "        spark_result = topics.collect()\n",
    "        final_aggregations(new_grid, spark_result)\n",
    "\n",
    "ssc = StreamingContext(sc, 30) #30 is the batch interval in seconds\n",
    "\n",
    "ip = \"localhost\"\n",
    "port = 5727\n",
    "\n",
    "lines = ssc.socketTextStream(ip, port)\n",
    "tweets = lines.filter(filter_tweet)\n",
    "tweets = tweets.map(evaluate_tweet)\n",
    "topics = tweets.reduceByKey(lambda a, b: lda_helper.merge_toptopic(a, b))\n",
    "topics.transform(lambda rdd: rdd.sortByKey(True))\n",
    "\n",
    "topics.foreachRDD( evaluate_rdd )\n",
    " \n",
    "ssc.start()\n",
    "ssc.awaitTermination()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
