{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>7</td><td>application_1501058268283_0012</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://hn1-bigurl.fyufg335a0aufcivtbliqhom2b.ax.internal.cloudapp.net:8088/proxy/application_1501058268283_0012/\">Link</a></td><td><a target=\"_blank\" href=\"http://10.0.0.6:30060/node/containerlogs/container_1501058268283_0012_01_000001/livy\">Link</a></td><td>âœ”</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    }
   ],
   "source": [
    "# Spark modules\n",
    "from pyspark import SparkConf, SparkContext, SparkFiles\n",
    "\n",
    "# Standard modules\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "# Third party modules\n",
    "import gensim\n",
    "import shapely\n",
    "\n",
    "# Definition of LDA Helper class\n",
    "# contains useful method to compute the topics\n",
    "# distribution and the top topic aggregation method\n",
    "class LDAHelper:\n",
    "\n",
    "    def __init__(self, ntopics):\n",
    "        self.ntopics = ntopics\n",
    "\n",
    "    def calculate_topic_distributions(self, model, documents):\n",
    "        \"\"\"\n",
    "        Input: model, documents (set of bow)\n",
    "        Output: a list with (topic_index, sum_of_distributions)\n",
    "        \"\"\"\n",
    "        top_topics = [0] * (self.ntopics)\n",
    "        for document in documents:\n",
    "            dist = model.get_document_topics(document, minimum_probability=0)\n",
    "            for doc_id in range(self.ntopics):\n",
    "                top_topics[doc_id] += dist[doc_id][1]\n",
    "\n",
    "        to_return = []\n",
    "        for i, top in enumerate(top_topics):\n",
    "            to_return.append((model.show_topic(i, 5), top))\n",
    "\n",
    "        return sorted(to_return, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    def merge_topic_lists(self, wtopics1, wtopics2):\n",
    "        \"\"\"\n",
    "        Merge two topic lists together.\n",
    "        If it finds the same topic in both lists,\n",
    "        it keeps only one of them with the sum of\n",
    "        the weights.\n",
    "        \"\"\"\n",
    "        new_topics = []\n",
    "        for wtopic1 in wtopics1:\n",
    "            topic1, weight_1 = wtopic1\n",
    "            new_w = weight_1\n",
    "            for wtopic2 in wtopics2:\n",
    "                topic2, weight_2 = wtopic2\n",
    "                if topic1 == topic2:\n",
    "                    new_w += weight_2\n",
    "                    break\n",
    "            new_topics.append((topic1, new_w))\n",
    "        return new_topics\n",
    "\n",
    "\n",
    "    def merge_toptopic(self, top_topic1, top_topic2):\n",
    "        merged_top_topics = self.merge_topic_lists(top_topic1, top_topic2)\n",
    "        merged_top_topics = sorted(merged_top_topics, key=lambda x: x[1], reverse=True)\n",
    "        return  merged_top_topics[:self.ntopics]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# LDA Helper\n",
    "lda_helper = LDAHelper(15)\n",
    "\n",
    "# Load the files from Azure HDFS\n",
    "new_grid_filename1 = \"input_1_new_grid_part1.pkl\"\n",
    "new_grid_filename2 = \"input_1_new_grid_part2.pkl\"\n",
    "lda_map_filename = \"input_2_lda_map.pkl\"\n",
    "hdfs_base_path = \"wasb:///mydata/\"\n",
    "\n",
    "sc.addFile(hdfs_base_path + new_grid_filename1)\n",
    "sc.addFile(hdfs_base_path + new_grid_filename2)\n",
    "sc.addFile(hdfs_base_path + lda_map_filename)\n",
    "\n",
    "# Get files back\n",
    "new_grid = []\n",
    "with open(SparkFiles.get(new_grid_filename1), 'rb') as fw:\n",
    "    new_grid = pickle.load(fw)\n",
    "\n",
    "with open(SparkFiles.get(new_grid_filename2), 'rb') as fw:\n",
    "    new_grid += pickle.load(fw)\n",
    "\n",
    "lda_map = None\n",
    "with open(SparkFiles.get(lda_map_filename), 'rb') as fw:\n",
    "    lda_map = pickle.load(fw)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Done:', 153.42076802253723, ' seconds')"
     ]
    }
   ],
   "source": [
    "# init: define spark_grid\n",
    "spark_grid = []\n",
    "counter = 0\n",
    "for cell in new_grid:\n",
    "    cell_id = counter\n",
    "    for p in cell['parts']:\n",
    "        corpus = p['corpus']\n",
    "        tup = (cell_id, corpus*10)\n",
    "        spark_grid.append(tup)\n",
    "    counter += 1\n",
    "\n",
    "# define lda_map brodcast variable\n",
    "# read-only shared variable\n",
    "lda_map_broadcast = sc.broadcast(lda_map)\n",
    "\n",
    "times = []\n",
    "start_time = time.time()\n",
    "\n",
    "# Spark Core\n",
    "rdd_grid = spark.sparkContext.parallelize(spark_grid)\n",
    "topics = rdd_grid.map(lambda scell: (scell[0], lda_helper.calculate_topic_distributions(lda_map_broadcast.value, scell[1])))\n",
    "topics = topics.reduceByKey(lambda a, b: lda_helper.merge_toptopic(a, b))\n",
    "topics = topics.sortByKey(True)\n",
    "spark_result = topics.collect()\n",
    "\n",
    "# Format the output\n",
    "spark_result = list(zip(*spark_result))[1]\n",
    "spark_result = list(spark_result)\n",
    "\n",
    "times.append(time.time() - start_time)\n",
    "print(\"Done:\", times[0], \" seconds\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 2
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
