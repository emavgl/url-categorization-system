{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Mining Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "import geopy\n",
    "import geopy.distance\n",
    "import pprint\n",
    "import pymongo\n",
    "import time\n",
    "from lib.lda import LDAHelper\n",
    "\n",
    "# Istantiate the helper object\n",
    "lda_helper = LDAHelper(disableLogs=True)\n",
    "\n",
    "# connect db\n",
    "client = pymongo.MongoClient('localhost', 27017)\n",
    "\n",
    "# get db\n",
    "db = client['url-project']\n",
    "\n",
    "# open collection\n",
    "documents = db['documents']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def move(start, direction, distance):\n",
    "    \"\"\"\n",
    "    Returns a geopy.Point distant \"distance\"\n",
    "    from the \"start\" point in the specified \"direction\"\n",
    "    \"\"\"\n",
    "    orientation = 0\n",
    "    start_point = copy.deepcopy(start)\n",
    "    if direction == 'r':\n",
    "        orientation = 90\n",
    "    elif direction == 'u':\n",
    "        orientation = 0\n",
    "    elif direction == 'l':\n",
    "        orientation = 360\n",
    "    elif direction == 'b':\n",
    "        orientation = 180\n",
    "    return distance.destination(point=start_point, bearing=orientation)\n",
    "\n",
    "def create_grid(bottom_left, upper_right, step):\n",
    "    \"\"\"\n",
    "    Creates a grid of size step*step\n",
    "    Each cells contains 4 geopy.Point which represent\n",
    "    the 4 corners of each cells.\n",
    "    \"\"\"\n",
    "    lat_bl, lon_bl = bottom_left\n",
    "    lat_ur, lon_ur = upper_right\n",
    "    bottom_left = geopy.Point(lat_bl, lon_bl)\n",
    "    bottom_right = geopy.Point(lat_bl, lon_ur)\n",
    "    upper_right = geopy.Point(lat_ur, lon_ur)\n",
    "    upper_left = geopy.Point(lat_ur, lon_bl)\n",
    "\n",
    "    # calculate cell size, the same if bottom_left and upper_right describe a square\n",
    "    distance_horizontal = geopy.distance.vincenty(bottom_left, bottom_right).meters\n",
    "    new_meters = distance_horizontal/step\n",
    "\n",
    "    # vertical size cells\n",
    "    dh = geopy.distance.VincentyDistance(meters=new_meters)\n",
    "\n",
    "    distance_vertical = geopy.distance.vincenty(upper_left, bottom_left).meters\n",
    "    new_meters = distance_vertical/step\n",
    "\n",
    "    # horizontal size cells\n",
    "    dv = geopy.distance.VincentyDistance(meters=new_meters)\n",
    "\n",
    "    # define a matrix step*step\n",
    "    # each cell has a list with 4 Points which represent the 4 corners\n",
    "    matrix = [[0]*step for i in range(step)]\n",
    "\n",
    "    tmp = move(upper_left, 'b', dv)\n",
    "    matrix[0][0] = [upper_left, upper_right, tmp, move(tmp, 'r', dh)]\n",
    "\n",
    "    # fill the first column\n",
    "    for i in range(1, step):\n",
    "        matrix[i][0] = [matrix[i-1][0][2], matrix[i-1][0][3],\n",
    "                        move(matrix[i-1][0][2], 'b', dv), move(matrix[i-1][0][3], 'b', dv)]\n",
    "\n",
    "    # fill all the other columns\n",
    "    for i in range(0, step):\n",
    "        for j in range(1, step):\n",
    "            matrix[i][j] = [matrix[i][j-1][1], move(matrix[i][j-1][1], 'r', dh),\n",
    "                            matrix[i][j-1][3], move(matrix[i][j-1][3], 'r', dh)]\n",
    "\n",
    "    return matrix\n",
    "\n",
    "\n",
    "def get_texts_from_db(mongo_collection, bottom_left_corner, upper_right_corner):\n",
    "    \"\"\"\n",
    "    Return list of strings that represents text of documents into that area\n",
    "    Input: mongo_collection, bottom_left_corner (lat, long), upper_right_corner (lat, long)\n",
    "    \"\"\"\n",
    "    results = mongo_collection.find({\n",
    "        'loc': {'$geoWithin': {'$box': [[bottom_left_corner[1], bottom_left_corner[0]],\n",
    "                                        [upper_right_corner[1], upper_right_corner[0]]]}}\n",
    "    })\n",
    "\n",
    "    corpus = []\n",
    "    for result in results:\n",
    "        corpus.append(result['boilerpipe'])\n",
    "\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creation of the minimum Grid\n",
    "First of all, the basic grid it is created. We consider as the minium grid a **4x4** grid. To create the grid:\n",
    "1. Get a geographic area, rect using bottom left and top right geopoints\n",
    "2. Call the functions **create_grid** that create a matrix of dimension **4x4**, each cells contains the geographic coords of the identified area (rect using bottom left and top right geopoints)\n",
    "3. For each cell, then retrieve from the database the documents in the corresponding geographic area.\n",
    "4. For each cells, define the corpus (tokenized documents)\n",
    "5. At the end, the object gensim.Dictionary is created.\n",
    "\n",
    "We are ready to run the baseline approach and compute the lda model for each cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Give as imput the bottom left corner and top right corner.\n",
    "# Divides the geographic map in a grid 4x4\n",
    "grid = create_grid((44.222765, 7.531128), (47.056277, 12.661743), 4)\n",
    "\n",
    "all_texts = []\n",
    "\n",
    "# iterate over each cells\n",
    "for i in range(len(grid)):\n",
    "    for j in range(len(grid)):\n",
    "        \n",
    "        # get the cell\n",
    "        cell = grid[i][j]\n",
    "        corners = copy.deepcopy(cell)\n",
    "\n",
    "        # get texts in that area\n",
    "        texts = get_texts_from_db(documents, corners[2], corners[1])\n",
    "\n",
    "        # tokenized documents\n",
    "        corpus = []\n",
    "        for text in texts:\n",
    "            corpus.append(lda_helper.clean(text))\n",
    "        \n",
    "        # merge tokenized documents to the set of all documents\n",
    "        all_texts += corpus\n",
    "\n",
    "        # define the object in the grid\n",
    "        grid[i][j] = {'corners': corners, 'texts': corpus}\n",
    "        \n",
    "dictionary = lda_helper.create_dictionary(all_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline\n",
    "#### Compute the lda model for each cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing...\n",
      "0 0 127\n",
      "computing...\n",
      "0 1 136\n",
      "computing...\n",
      "0 2 126\n",
      "computing...\n",
      "0 3 36\n",
      "computing...\n",
      "1 0 163\n",
      "computing...\n",
      "1 1 602\n",
      "computing...\n",
      "1 2 153\n",
      "computing...\n",
      "1 3 354\n",
      "computing...\n",
      "2 0 924\n",
      "computing...\n",
      "2 1 1897\n",
      "computing...\n",
      "2 2 679\n",
      "computing...\n",
      "2 3 501\n",
      "computing...\n",
      "3 0 377\n",
      "computing...\n",
      "3 1 198\n",
      "computing...\n",
      "3 2 712\n",
      "computing...\n",
      "3 3 362\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(grid)):\n",
    "    for j in range(len(grid)):\n",
    "        \n",
    "        # get the cell\n",
    "        cell = grid[i][j]\n",
    "        \n",
    "        # get tokenized documents\n",
    "        texts = cell['texts']\n",
    "        \n",
    "        print(\"computing...\")\n",
    "        print(i, j, len(texts))\n",
    "        \n",
    "        # compute lda model\n",
    "        lda_results = lda_helper.lda_topic(texts, dictionary=dictionary, dictionary_filters=False)\n",
    "        \n",
    "        # get lda results: model, topics and corpus (doc2bow)\n",
    "        cell['lda_model'] = lda_results['lda_model']\n",
    "        cell['topics'] = lda_results['topics']\n",
    "        cell['corpus'] = lda_results['corpus']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('data/minimalgrid.pkl', 'wb') as f:\n",
    "    pickle.dump(grid, f)\n",
    "\n",
    "# load pre-computated minimal-grid 4x4\n",
    "with open('data/minimalgrid.pkl', 'rb') as f:\n",
    "    minimalgrid = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Change Grid size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0\n",
      "0 0\n",
      "0 1\n",
      "1 0\n",
      "1 1\n",
      "0 2\n",
      "0 3\n",
      "1 2\n",
      "1 3\n",
      "2 0\n",
      "2 1\n",
      "3 0\n",
      "3 1\n",
      "2 2\n",
      "2 3\n",
      "3 2\n",
      "3 3\n"
     ]
    }
   ],
   "source": [
    "# minimalgrid 4x4 -> grid 2x2\n",
    "preceed_grid = minimalgrid\n",
    "preceed_dimension = 4\n",
    "target_dimension = 2\n",
    "new_cell_side = preceed_dimension / target_dimension\n",
    "print(new_cell_side)\n",
    "\n",
    "def getSquare(m, i, j, new_side):\n",
    "    sq = []\n",
    "    \n",
    "    # upper left indexes\n",
    "    i_up_index = int(i*new_side)\n",
    "    j_up_index = int(j*new_side)\n",
    "    new_side = int(new_side)\n",
    "    \n",
    "    for i in range(i_up_index, i_up_index + new_side):\n",
    "        for j in range(j_up_index, j_up_index + new_side):\n",
    "            print(i, j)\n",
    "            sq.append(m[i][j])\n",
    "        \n",
    "    return sq\n",
    "\n",
    "new_grid = [[0]*target_dimension for i in range(target_dimension)]\n",
    "for i in range(target_dimension):\n",
    "    for j in range(target_dimension):\n",
    "        new_grid[i][j] = getSquare(preceed_grid, i, j, new_cell_side)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 222.67525029182434 seconds ---\n"
     ]
    }
   ],
   "source": [
    "### recompute new_grid\n",
    "def computeBaseline(grid):\n",
    "    side = len(grid)\n",
    "    \n",
    "    for i in range(side):\n",
    "        for j in range(side):\n",
    "            parts = grid[i][j]\n",
    "            texts = []\n",
    "            for p in parts:\n",
    "                texts += p[\"texts\"]\n",
    "            \n",
    "            # compute lda model\n",
    "            lda_results = lda_helper.lda_topic(texts, dictionary=dictionary, dictionary_filters=False)\n",
    "\n",
    "            grid[i][j] = {'texts': texts}\n",
    "            grid[i][j]['corpus'] = lda_results['corpus']\n",
    "            grid[i][j]['lda_model'] = lda_results['lda_model']\n",
    "            grid[i][j]['topics'] = lda_results['topics']\n",
    "            \n",
    "\n",
    "new_baseline_grid = copy.deepcopy(new_grid)\n",
    "start_time = time.time()\n",
    "computeBaseline(new_baseline_grid)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 80.52158284187317 seconds ---\n"
     ]
    }
   ],
   "source": [
    "### lda update new_grid\n",
    "def pickTheBestOne(parts):\n",
    "    max_len = 0\n",
    "    best_idx = 0\n",
    "    \n",
    "    for i, p in enumerate(parts):\n",
    "        current = len(p['texts'])\n",
    "        if current > max_len:\n",
    "            max_len = current\n",
    "            best_idx = i\n",
    "    \n",
    "    best_part = parts.pop(best_idx)\n",
    "    return best_part, parts\n",
    "\n",
    "def computeLdaUpdate(grid):\n",
    "    side = len(grid)\n",
    "    for i in range(side):\n",
    "        for j in range(side):\n",
    "            parts = grid[i][j]\n",
    "            best_part, rest = pickTheBestOne(parts)\n",
    "            lda_model = best_part['lda_model']\n",
    "            corpus = []\n",
    "            texts = [best_part[\"texts\"]]\n",
    "            for p in rest:\n",
    "                corpus += p[\"corpus\"]\n",
    "                texts += p[\"texts\"]\n",
    "            total_corpus = corpus + best_part['corpus']\n",
    "            lda_results = lda_helper.lda_update_merge(lda_model, corpus, total_corpus)\n",
    "            grid[i][j] = {'corpus': total_corpus, 'texts': texts}\n",
    "            grid[i][j]['lda_model'] = lda_results['lda_model']\n",
    "            grid[i][j]['topics'] = lda_results['topics']\n",
    "\n",
    "grid_lda_update = copy.deepcopy(new_grid)\n",
    "start_time = time.time()\n",
    "computeLdaUpdate(grid_lda_update)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
